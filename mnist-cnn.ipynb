{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aff80fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2016711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.x, self.y = torch.load(filepath)\n",
    "        self.x = self.x / 255.\n",
    "        self.y = F.one_hot(self.y, num_classes=10).to(float)\n",
    "    def __len__(self): \n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, ix): \n",
    "        return self.x[ix], self.y[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1274d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDS = CTDataset('C:\\\\Users\\\\ashul\\\\Desktop\\\\handwriting files\\\\MNIST\\\\MNIST\\\\processed\\\\training.pt')\n",
    "TestDS = CTDataset('C:\\\\Users\\\\ashul\\\\Desktop\\\\handwriting files\\\\MNIST\\\\MNIST\\\\processed\\\\test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b4ef299",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 8\n",
    "TrainDL = DataLoader(TrainDS,batch_size=BatchSize, shuffle=True)\n",
    "TestDL = DataLoader(TestDS,batch_size=BatchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0780476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,InChannels=1, Classes=10):\n",
    "        super().__init__()\n",
    "        MatchingChannels = 20\n",
    "        self.conv1 = nn.Conv2d(in_channels=InChannels,out_channels=MatchingChannels,kernel_size=(5,5)) #consider adding padding?\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=MatchingChannels,out_channels=50,kernel_size=(5,5))\n",
    "        self.fc1 = nn.Linear(in_features = 800, out_features = 500)\n",
    "        self.fc2 = nn.Linear(in_features = 500, out_features = Classes)\n",
    "        self.LSM = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        output = self.LSM(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e149873",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = datasets.MNIST(root=\"\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "Test = datasets.MNIST(root=\"\", train=False, transform=transforms.ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "90f16a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "BatchSize = 64\n",
    "Epochs = 10\n",
    "\n",
    "TrainSplit = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f6b15bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumTrainSamples = int(len(Train)*TrainSplit)\n",
    "NumValSamples = len(Train) - NumTrainSamples\n",
    "(TrainData, ValData) = random_split(Train,[NumTrainSamples,NumValSamples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c5fef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataLoader = DataLoader(TrainData, shuffle=True, batch_size = BatchSize)\n",
    "ValDataLoader = DataLoader(ValData, batch_size = BatchSize)\n",
    "TestDataLoader = DataLoader(Test, batch_size = BatchSize)\n",
    "\n",
    "TrainSteps = len(TrainDataLoader.dataset) // BatchSize\n",
    "ValSteps = len(ValDataLoader.dataset) // BatchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9506b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/\n",
    "model = CNN()\n",
    "opt = Adam(model.parameters(), lr=lr)\n",
    "L = nn.NLLLoss()\n",
    "\n",
    "H = {\n",
    "    \"TrainLoss\": [],\n",
    "    \"TrainAccuracy\": [],\n",
    "    \"ValLoss\": [],\n",
    "    \"ValAccuracy\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4d9fdb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/10\n",
      "Train loss: 0.174229, Train accuracy: 0.9461\n",
      "Val loss: 0.061431, Val accuracy: 0.9822\n",
      "\n",
      "[INFO] EPOCH: 2/10\n",
      "Train loss: 0.047933, Train accuracy: 0.9852\n",
      "Val loss: 0.051430, Val accuracy: 0.9834\n",
      "\n",
      "[INFO] EPOCH: 3/10\n",
      "Train loss: 0.034575, Train accuracy: 0.9891\n",
      "Val loss: 0.042619, Val accuracy: 0.9864\n",
      "\n",
      "[INFO] EPOCH: 4/10\n",
      "Train loss: 0.025879, Train accuracy: 0.9918\n",
      "Val loss: 0.036919, Val accuracy: 0.9882\n",
      "\n",
      "[INFO] EPOCH: 5/10\n",
      "Train loss: 0.018671, Train accuracy: 0.9937\n",
      "Val loss: 0.042121, Val accuracy: 0.9861\n",
      "\n",
      "[INFO] EPOCH: 6/10\n",
      "Train loss: 0.015139, Train accuracy: 0.9954\n",
      "Val loss: 0.033792, Val accuracy: 0.9906\n",
      "\n",
      "[INFO] EPOCH: 7/10\n",
      "Train loss: 0.013882, Train accuracy: 0.9953\n",
      "Val loss: 0.031063, Val accuracy: 0.9912\n",
      "\n",
      "[INFO] EPOCH: 8/10\n",
      "Train loss: 0.011490, Train accuracy: 0.9964\n",
      "Val loss: 0.048155, Val accuracy: 0.9865\n",
      "\n",
      "[INFO] EPOCH: 9/10\n",
      "Train loss: 0.010371, Train accuracy: 0.9965\n",
      "Val loss: 0.030615, Val accuracy: 0.9910\n",
      "\n",
      "[INFO] EPOCH: 10/10\n",
      "Train loss: 0.007352, Train accuracy: 0.9977\n",
      "Val loss: 0.040457, Val accuracy: 0.9903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(Epochs):\n",
    "    model.train() #set model in training mode\n",
    "    \n",
    "    TotalTrainLoss = 0\n",
    "    TotalValLoss = 0\n",
    "    \n",
    "    TrainCorrect = 0\n",
    "    ValCorrect = 0\n",
    "    \n",
    "    for (x,y) in TrainDataLoader:\n",
    "        \n",
    "        #forward pass\n",
    "        pred = model(x)\n",
    "        loss = L(pred,y)\n",
    "        \n",
    "        #backward pass/backpropagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        TotalTrainLoss += loss\n",
    "        TrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        \n",
    "    #now we turn evaluation off to see how the network did\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # loop over the validation set\n",
    "        for (x, y) in ValDataLoader:\n",
    "            \n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            TotalValLoss += L(pred, y)\n",
    "            \n",
    "            # calculate the number of correct predictions\n",
    "            ValCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = TotalTrainLoss / TrainSteps\n",
    "    avgValLoss = TotalValLoss / ValSteps\n",
    "    \n",
    "    # calculate the training and validation accuracy\n",
    "    TrainCorrect = TrainCorrect / len(TrainDataLoader.dataset)\n",
    "    ValCorrect = ValCorrect / len(ValDataLoader.dataset)\n",
    "    \n",
    "    # update our training history\n",
    "    H[\"TrainLoss\"].append(avgTrainLoss)\n",
    "    H[\"TrainAccuracy\"].append(TrainCorrect)\n",
    "    H[\"ValLoss\"].append(avgValLoss)\n",
    "    H[\"ValAccuracy\"].append(ValCorrect)\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, Epochs))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "        avgTrainLoss, TrainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "        avgValLoss, ValCorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "addff9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"CNN-weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b221c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "019157df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(28):\\n    for j in range(28):\\n        print(i,j,pix[i,j])\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestImage = Image.open(\"C:\\\\Users\\\\ashul\\\\Downloads\\\\testing.png\").convert(\"L\") #the \"L\" converts to grayscale\n",
    "pix = TestImage.load()\n",
    "print(TestImage.size)\n",
    "\"\"\"\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        print(i,j,pix[i,j])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "de341a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert PIL  \n",
    "# image to a Torch tensor \n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "])\n",
    "ImageTensor = transform(TestImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ddaa1bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          215, 133, 216, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 219,\n",
      "           97,  84, 174, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 223, 100,\n",
      "           84, 144, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 238, 110,  84,\n",
      "          140, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 126,  84, 130,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 167,  84, 112, 242,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 214,  84,  84, 209, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 133,  84, 165, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 200,  84,  98, 248, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 127,  84, 180, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 215,  84,  97, 253, 255, 255, 235,\n",
      "          196, 191, 202, 213, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 162,  84, 164, 255, 186, 114,  84,\n",
      "           84,  84,  84,  84, 122, 199, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 125,  84, 207, 116,  84,  84,  89,\n",
      "          126, 140, 134, 107,  84,  89, 238, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 100,  84,  93,  84, 121, 192, 255,\n",
      "          255, 255, 255, 255, 119,  84, 204, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 121,  84,  84, 147, 254, 255, 255,\n",
      "          255, 255, 255, 195,  84,  98, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 235,  84,  84, 124, 205, 255, 255, 255,\n",
      "          248, 210, 154,  84,  84, 165, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 177, 116,  84,  84,  95, 102, 105,\n",
      "           84,  84,  84,  84, 167, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 242, 172, 114,  84,  84,  84,\n",
      "           88, 114, 167, 224, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 245, 237, 232,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "          255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255]]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(ImageTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a8578f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageTensor = (ImageTensor - 255)/255\n",
    "ImageTensor.shape\n",
    "ITS = torch.unsqueeze(ImageTensor,0)\n",
    "ITS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6e2282d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhatTest = model(ITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8fd745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3625, -2.1787, -2.4219, -2.3168, -2.2947, -2.3676, -2.4120, -2.2945,\n",
       "         -2.0823, -2.3465]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "30bf4d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatTest.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "55e2455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.7519, -12.3841,  -7.2004,  -5.2239, -17.8147,  -4.0397,  -9.8912,\n",
      "         -12.2822,  -0.6837, -10.9050]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestImage = Image.open(\"C:\\\\Users\\\\ashul\\\\Downloads\\\\testing.png\").convert(\"L\") #the \"L\" converts to grayscale\n",
    "ImageTensor = transform(TestImage)\n",
    "ImageTensor = (ImageTensor - 255)/255\n",
    "yhatTest = model(torch.unsqueeze(ImageTensor,0))\n",
    "print(yhatTest)\n",
    "torch.argmax(yhatTest).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "02fa5391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([[-6.9486e+00, -3.0965e-02, -5.9770e+00, -1.9729e+01, -9.0898e+00,\n",
      "         -9.7209e+00, -4.4155e+00, -1.4942e+01, -4.2178e+00, -1.3024e+01],\n",
      "        [-3.5193e+01, -2.3461e+01, -2.7872e+01, -3.7042e+01,  0.0000e+00,\n",
      "         -2.6009e+01, -2.8300e+01, -2.5913e+01, -1.7565e+01, -2.1769e+01],\n",
      "        [-3.4664e+01, -2.3567e+01, -2.6564e+01, -3.4402e+01,  0.0000e+00,\n",
      "         -2.6859e+01, -2.8429e+01, -3.1124e+01, -2.6984e+01, -3.2013e+01],\n",
      "        [-1.3169e+01, -2.1458e-06, -1.7925e+01, -2.8198e+01, -1.7376e+01,\n",
      "         -1.5686e+01, -1.8720e+01, -1.8243e+01, -1.5651e+01, -1.7980e+01],\n",
      "        [-3.4580e+01, -2.3746e+01, -3.5154e+01, -1.3983e+01, -3.8151e+01,\n",
      "         -8.3446e-07, -3.4539e+01, -2.8566e+01, -2.6760e+01, -2.7492e+01],\n",
      "        [-3.5196e-04, -2.7142e+01, -1.8205e+01, -1.8901e+01, -2.2832e+01,\n",
      "         -1.8916e+01, -7.9535e+00, -2.8580e+01, -1.4613e+01, -1.8150e+01],\n",
      "        [-2.6762e+01, -4.0136e+01, -4.4411e+01, -4.0748e+01, -3.3003e+01,\n",
      "         -2.5474e+01,  0.0000e+00, -5.0173e+01, -2.4125e+01, -4.5011e+01],\n",
      "        [-3.9072e+01, -5.1831e+01, -3.4190e+01, -4.7478e+01, -3.7507e+01,\n",
      "         -3.2344e+01,  0.0000e+00, -6.7439e+01, -3.4437e+01, -5.7966e+01],\n",
      "        [-1.5930e+01, -1.4901e-05, -1.5218e+01, -2.5841e+01, -1.5654e+01,\n",
      "         -1.5488e+01, -1.8261e+01, -1.1177e+01, -1.5372e+01, -1.9487e+01],\n",
      "        [-3.1393e+01, -1.7470e+01, -2.3639e+01, -2.3842e-07, -3.0003e+01,\n",
      "         -1.5257e+01, -2.8644e+01, -2.1661e+01, -1.8356e+01, -2.5517e+01],\n",
      "        [-2.2901e+01, -1.8821e+01, -2.2894e+01, -1.2278e-05, -2.9719e+01,\n",
      "         -1.1362e+01, -2.4532e+01, -2.4083e+01, -1.4351e+01, -1.9963e+01],\n",
      "        [-3.5075e+01, -2.6871e+01, -2.6404e+01, -3.1473e+01, -3.7711e+01,\n",
      "         -3.3875e+01, -3.9856e+01, -3.2762e+01,  0.0000e+00, -3.2608e+01],\n",
      "        [-4.4778e+01, -2.9615e+01, -4.2079e+01,  0.0000e+00, -5.2773e+01,\n",
      "         -2.4532e+01, -4.9444e+01, -3.8498e+01, -3.2383e+01, -4.2203e+01],\n",
      "        [-2.2264e+01, -1.6929e+01,  0.0000e+00, -2.9443e+01, -3.2015e+01,\n",
      "         -4.1108e+01, -3.3126e+01, -1.7115e+01, -2.4061e+01, -3.2864e+01],\n",
      "        [-2.5041e+01, -3.7811e+01, -3.9299e+01, -3.5969e+01, -2.9514e+01,\n",
      "         -3.0305e+01,  0.0000e+00, -4.8854e+01, -2.6342e+01, -4.1057e+01],\n",
      "        [-1.1484e+01, -1.3609e+01, -1.2600e+01, -1.9584e-02, -1.5521e+01,\n",
      "         -9.2062e+00, -1.8624e+01, -1.2059e+01, -1.1881e+01, -3.9495e+00],\n",
      "        [-3.2077e+01, -3.6588e+01, -3.9147e+01, -3.8776e+01, -3.2971e+01,\n",
      "         -2.1728e+01,  0.0000e+00, -5.0005e+01, -2.9750e+01, -4.6430e+01],\n",
      "        [ 0.0000e+00, -3.1065e+01, -1.7363e+01, -2.4761e+01, -3.0534e+01,\n",
      "         -1.9184e+01, -2.1095e+01, -2.8362e+01, -2.0901e+01, -2.5412e+01],\n",
      "        [-3.1400e+01, -3.5107e+01, -2.7432e+01, -3.0091e+01, -1.5794e+01,\n",
      "         -2.5241e+01, -3.6511e+01, -2.4197e+01, -1.2749e+01, -2.9802e-06],\n",
      "        [-1.8085e+01, -3.5763e-07, -1.8006e+01, -3.1616e+01, -1.9701e+01,\n",
      "         -1.9702e+01, -2.1382e+01, -1.4777e+01, -2.0153e+01, -2.5387e+01],\n",
      "        [-3.0035e+01, -2.6286e+01, -2.1316e+01, -2.1186e+01, -1.7206e+01,\n",
      "         -2.2308e+01, -3.1567e+01, -2.8967e-05, -2.6583e+01, -1.0451e+01],\n",
      "        [-4.5845e+01, -3.9139e+01, -3.3802e+01, -3.3888e+01, -4.9141e+01,\n",
      "         -3.8905e+01, -5.0001e+01, -4.0334e+01,  0.0000e+00, -3.7387e+01],\n",
      "        [-3.0818e+01, -1.6492e+01, -1.9032e+01, -3.0189e+01, -1.1921e-07,\n",
      "         -1.9947e+01, -2.0660e+01, -2.1109e+01, -1.8077e+01, -2.2446e+01],\n",
      "        [-1.9314e+01, -8.5143e-03, -1.0929e+01, -1.8717e+01, -1.8317e+01,\n",
      "         -2.2269e+01, -2.5008e+01, -4.7724e+00, -2.2175e+01, -2.2986e+01],\n",
      "        [-2.2764e+01, -2.7120e+01, -2.0310e+01, -3.0814e+01, -7.4324e+00,\n",
      "         -2.3995e+01, -2.2586e+01, -2.3756e+01, -1.8603e+01, -5.9194e-04],\n",
      "        [-2.8118e+01, -2.8179e+01, -1.9732e+01, -2.9904e+01,  0.0000e+00,\n",
      "         -2.7398e+01, -2.2155e+01, -3.5621e+01, -3.2813e+01, -2.2134e+01],\n",
      "        [-4.5648e+01, -3.0162e+01, -3.8304e+01,  0.0000e+00, -4.8670e+01,\n",
      "         -2.7023e+01, -5.1696e+01, -3.1140e+01, -2.8879e+01, -3.3815e+01],\n",
      "        [-3.4042e+01, -2.2506e+01, -2.6816e+01, -3.9389e+01,  0.0000e+00,\n",
      "         -3.1394e+01, -2.5962e+01, -2.7299e+01, -3.2621e+01, -2.7384e+01],\n",
      "        [ 0.0000e+00, -3.7238e+01, -2.7804e+01, -4.1741e+01, -4.5036e+01,\n",
      "         -4.1339e+01, -2.9665e+01, -3.8640e+01, -3.3028e+01, -3.4153e+01],\n",
      "        [-1.7839e+01, -1.1921e-07, -1.8353e+01, -3.5164e+01, -1.6172e+01,\n",
      "         -1.7059e+01, -1.8263e+01, -2.1632e+01, -1.7816e+01, -2.4822e+01],\n",
      "        [-2.3702e+01, -2.3994e+01, -2.0084e+01, -2.9456e+01, -3.1356e+01,\n",
      "         -2.6615e+01, -2.9886e+01, -2.8278e+01,  0.0000e+00, -2.5273e+01],\n",
      "        [-1.4205e+01, -2.4160e+01, -7.8372e+00, -9.6628e+00, -1.5423e+01,\n",
      "         -9.5308e+00, -2.8096e+01, -1.7488e+01, -8.4212e+00, -7.5229e-04],\n",
      "        [-1.5374e+01, -2.3246e-05, -1.9759e+01, -3.0623e+01, -1.1593e+01,\n",
      "         -1.1936e+01, -1.8344e+01, -1.3602e+01, -1.1997e+01, -1.6636e+01],\n",
      "        [-3.4866e+01, -3.2824e+01, -3.5937e+01, -1.9963e+01, -2.8224e+01,\n",
      "          0.0000e+00, -2.7072e+01, -3.4586e+01, -2.2995e+01, -2.1984e+01],\n",
      "        [-2.9954e+01, -1.7114e+01, -2.0682e+01, -6.3181e-06, -2.8625e+01,\n",
      "         -1.1989e+01, -2.7815e+01, -1.7419e+01, -1.5911e+01, -2.1411e+01],\n",
      "        [-2.8135e+01, -3.4494e+01, -2.3398e+01, -2.0063e+01, -1.8412e+01,\n",
      "         -2.0076e+01, -3.2774e+01, -2.1464e+01, -1.1116e+01, -1.4901e-05],\n",
      "        [-4.2699e+01, -1.9731e+01, -2.5241e+01,  0.0000e+00, -3.4258e+01,\n",
      "         -2.1900e+01, -3.8963e+01, -2.0926e+01, -2.2701e+01, -3.1746e+01],\n",
      "        [-2.7019e+01, -3.1022e+01, -3.2127e+01, -1.8712e+01, -2.7343e+01,\n",
      "         -3.5763e-07, -3.1265e+01, -3.1669e+01, -2.4690e+01, -1.4723e+01],\n",
      "        [-1.7267e+01,  0.0000e+00, -1.9578e+01, -3.5258e+01, -1.7035e+01,\n",
      "         -1.8605e+01, -2.1175e+01, -1.9700e+01, -1.8087e+01, -2.4342e+01],\n",
      "        [-1.3479e+01, -3.4025e+01, -2.5191e+01, -3.4126e+01, -2.5690e+01,\n",
      "         -2.1513e+01, -1.4305e-06, -4.3440e+01, -1.9868e+01, -3.0346e+01],\n",
      "        [-1.4355e+01, -1.2978e+01, -1.3085e+01, -1.5531e+01, -6.6297e+00,\n",
      "         -9.4524e+00, -1.8118e+01, -1.1867e+01, -8.8989e+00, -1.5490e-03],\n",
      "        [-1.7926e+01, -1.1921e-07, -1.9680e+01, -3.3576e+01, -1.5862e+01,\n",
      "         -1.7978e+01, -2.0997e+01, -1.8088e+01, -1.8541e+01, -2.3910e+01],\n",
      "        [ 0.0000e+00, -3.1665e+01, -2.1933e+01, -3.3924e+01, -3.6068e+01,\n",
      "         -3.2708e+01, -2.1828e+01, -3.2280e+01, -2.7880e+01, -2.4345e+01],\n",
      "        [-4.4304e+01, -3.8175e+01, -2.9385e+01, -3.7128e+01, -4.8401e+01,\n",
      "         -3.9440e+01, -5.2232e+01, -4.0514e+01,  0.0000e+00, -4.0692e+01],\n",
      "        [-2.3806e+01, -2.3744e+01, -2.7380e+01, -2.8318e+01, -1.1754e+01,\n",
      "         -1.8180e+01, -2.5311e+01, -1.7172e+01, -2.5758e-04, -8.2955e+00],\n",
      "        [-2.1840e+01, -1.7411e+01, -1.5124e+01, -1.4581e+01, -1.4833e+01,\n",
      "         -1.8972e+01, -2.0145e+01, -1.3113e-06, -1.6644e+01, -1.6274e+01],\n",
      "        [-4.6492e-06, -2.7745e+01, -1.9404e+01, -1.6927e+01, -2.1663e+01,\n",
      "         -2.1204e+01, -1.2342e+01, -2.7586e+01, -1.5728e+01, -1.5618e+01],\n",
      "        [-2.7315e+01, -2.8937e+01, -3.3617e+01, -1.9373e+01, -2.1970e+01,\n",
      "          0.0000e+00, -1.6855e+01, -3.4166e+01, -1.7636e+01, -2.1331e+01],\n",
      "        [-3.6916e+01, -2.7899e+01, -2.5968e+01, -2.7234e+01, -3.5139e+01,\n",
      "         -2.8614e+01, -3.4833e+01, -3.2322e+01,  0.0000e+00, -3.4390e+01],\n",
      "        [-2.2051e+01,  0.0000e+00, -2.5388e+01, -3.4244e+01, -2.0520e+01,\n",
      "         -1.9907e+01, -2.3214e+01, -1.8258e+01, -2.0999e+01, -2.6339e+01],\n",
      "        [-3.1602e+01, -3.6756e+01, -3.3806e+01, -2.3185e+01, -2.8279e+01,\n",
      "          0.0000e+00, -3.1430e+01, -3.3614e+01, -2.3030e+01, -1.6651e+01],\n",
      "        [-2.5349e+01, -2.9189e+01, -3.6186e+01, -9.5781e+00, -3.0854e+01,\n",
      "         -6.9258e-05, -2.6289e+01, -3.6445e+01, -2.1605e+01, -2.2758e+01],\n",
      "        [-2.4217e+01, -2.2284e+01,  0.0000e+00, -3.3215e+01, -3.3432e+01,\n",
      "         -3.0365e+01, -3.6258e+01, -3.1781e+01, -2.2042e+01, -3.7583e+01],\n",
      "        [-1.7760e+01, -1.4629e+01, -5.9605e-07, -2.3665e+01, -2.2746e+01,\n",
      "         -2.8620e+01, -2.1676e+01, -1.5736e+01, -2.4301e+01, -2.7692e+01],\n",
      "        [-1.9165e+01, -2.0715e+01, -6.6159e-05, -1.8482e+01, -9.6359e+00,\n",
      "         -1.7249e+01, -1.9286e+01, -2.1809e+01, -1.3954e+01, -2.7627e+01],\n",
      "        [-2.7590e+01, -2.1285e+01, -1.9912e+01, -2.2297e+01, -2.1212e+01,\n",
      "         -2.5076e+01, -3.3036e+01,  0.0000e+00, -2.6784e+01, -2.1297e+01],\n",
      "        [ 0.0000e+00, -3.3432e+01, -2.5331e+01, -4.2354e+01, -3.8834e+01,\n",
      "         -3.7239e+01, -2.1573e+01, -3.6779e+01, -2.9216e+01, -2.9244e+01],\n",
      "        [-1.3778e+01, -2.0265e-05, -1.2387e+01, -2.3159e+01, -1.1590e+01,\n",
      "         -1.3584e+01, -1.7134e+01, -1.2655e+01, -1.3640e+01, -1.6833e+01],\n",
      "        [-1.2932e+01, -4.5299e-06, -1.3294e+01, -2.8717e+01, -1.8473e+01,\n",
      "         -1.9771e+01, -1.8901e+01, -1.4926e+01, -1.7075e+01, -2.2691e+01],\n",
      "        [-1.3835e+01, -2.0266e-06, -1.3896e+01, -2.9781e+01, -1.7232e+01,\n",
      "         -1.8426e+01, -1.9078e+01, -1.6277e+01, -1.7721e+01, -2.2086e+01],\n",
      "        [-2.2650e-06, -2.1084e+01, -1.3048e+01, -1.9339e+01, -2.6549e+01,\n",
      "         -2.0705e+01, -1.6087e+01, -2.3335e+01, -1.7183e+01, -2.2779e+01],\n",
      "        [-2.9953e+01, -3.6334e+01, -2.7146e+01, -3.1614e+01, -2.5237e+01,\n",
      "         -2.0014e+01,  0.0000e+00, -5.1865e+01, -3.1012e+01, -4.3749e+01],\n",
      "        [-4.0398e+01, -2.6496e+01, -2.7094e+01,  0.0000e+00, -3.9873e+01,\n",
      "         -2.2335e+01, -4.2037e+01, -2.2476e+01, -2.1217e+01, -2.7403e+01],\n",
      "        [-3.9418e+01, -3.1012e+01, -2.4146e+01, -2.9273e+01, -4.2753e+01,\n",
      "         -3.4370e+01, -4.0794e+01, -3.8458e+01,  0.0000e+00, -4.1081e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in ValDataLoader:\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(model(x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d0854b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1,2,3]\n",
    "T = torch.tensor(X)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91919faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7a3445f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.unsqueeze(T,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4b5a3ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b2c11604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38977a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
